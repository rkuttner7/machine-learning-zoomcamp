{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26336b19",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Workshop  \n",
    "\n",
    "build an image classification model using PyTorch and transfer learning\n",
    "  \n",
    "* [Deep Learning with PyTorch: Build, Train and Deploy an Image Classifier | Step-by-Step Tutorial](https://www.youtube.com/watch?v=Ne25VujHRLA) how to build an image classification model in PyTorch youtube video.\n",
    "  \n",
    "* [instructor's Notebook | Google Colab](https://colab.research.google.com/drive/1nCA4Q0f8DVFiLpfXdXvZtYUh-yYDy5i_?usp=sharing)\n",
    "\n",
    "* [workshop notes](https://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/08-deep-learning/pytorch)  \n",
    "\n",
    "\n",
    "Tools:\n",
    "\n",
    "    PyTorch\n",
    "    torchvision\n",
    "    PIL (Pillow)\n",
    "    NumPy\n",
    "\n",
    "PyTorch is a popular open-source deep learning framework developed by Facebook's AI Research lab. It provides:\n",
    "- Dynamic computation graphs (define-by-run)\n",
    "- Pythonic API\n",
    "- Strong GPU acceleration\n",
    "- Rich ecosystem of tools and libraries\n",
    "\n",
    "Key Differences from TensorFlow/Keras:\n",
    "\n",
    "| TensorFlow/Keras | PyTorch |\n",
    "|------------------|---------|\n",
    "| `model.fit()` | Manual training loop |\n",
    "| `ImageDataGenerator` | `Dataset` + `DataLoader` + `transforms` |\n",
    "| `keras.layers.Dense()` | `nn.Linear()` |\n",
    "| `keras.Model` | `nn.Module` |\n",
    "| `.h5` or `.keras` files | `.pth` or `.pt` files |\n",
    "\n",
    "\n",
    "| Concept | TensorFlow/Keras | PyTorch |\n",
    "|---------|------------------|---------|\n",
    "| Framework | High-level API (Keras) on TensorFlow | Low-level, explicit control |\n",
    "| Data Loading | `ImageDataGenerator` | `Dataset` + `DataLoader` |\n",
    "| Transforms | `preprocessing_function` | `transforms.Compose()` |\n",
    "| Model | Functional API or Sequential | `nn.Module` class |\n",
    "| Layers | `keras.layers.Dense()` | `nn.Linear()` |\n",
    "| Training | `model.fit()` | Manual training loop |\n",
    "| Loss | `CategoricalCrossentropy` | `CrossEntropyLoss` |\n",
    "| Optimizer | `keras.optimizers.Adam` | `optim.Adam` |\n",
    "| Saving | `.h5` or `.keras` | `.pth` or `.pt` |\n",
    "| Checkpointing | `ModelCheckpoint` callback | Manual in training loop |\n",
    "| Device | Automatic | Explicit `.to(device)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da501b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2699a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading and Preprocessing Images\n",
    "#-- Images are represented as 3D arrays:\n",
    "# - Height × Width × Channels\n",
    "# - Channels: RGB (Red, Green, Blue)\n",
    "# - Each channel: 8 bits (0-255 values)\n",
    "img = Image.open('clothing-dataset-small/train/pants/0098b991-e36e-4ef1-b5ee-4154b21e2a92.jpg')\n",
    "# Resize to target size\n",
    "img.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b073f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "x = np.array(img)\n",
    "print(x.shape)  # (224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b193826",
   "metadata": {},
   "source": [
    "### Using pre-trained model\n",
    "\n",
    "use pre-trained models:  \n",
    "- Already learned to recognize edges, textures, shapes\n",
    "- Saves training time\n",
    "- Works well even with small datasets\n",
    "- Better performance than training from scratc\n",
    "\n",
    "We'll use MobileNetV2 (in the original tutorial we used Xception):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144225c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "model.eval() # evaluation mode for making predictions (as opposed to `model.training()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8393ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8394e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = preprocess(img)\n",
    "# batch of size 1\n",
    "batch_t = torch.unsqueeze(img_t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1967977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction (batch size x class)\n",
    "with torch.no_grad(): # turn off training calculation to reduce memory consumption for predictions\n",
    "    output = model(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top predictions\n",
    "_, indices = torch.sort(output, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the classes\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07425e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNet class names\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top5_indices = indices[0, :5].tolist()\n",
    "top5_classes = [categories[i] for i in top5_indices]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, class_name in enumerate(top5_classes):\n",
    "    print(f\"{i+1}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd9d3f",
   "metadata": {},
   "source": [
    "### Key concepts:\n",
    "- Input size: MobileNetV2 expects 224×224 images (Xception uses 299×299)\n",
    "- Normalization: Images scaled with ImageNet mean and std\n",
    "- Batch size: Number of images processed together\n",
    "- Batch dimension: Shape (batch_size, channels, height, width) - e.g., (1, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3dfde6",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are specialized neural networks for processing grid-like data such as images.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "1. Convolutional Layer: Extracts features using filters\n",
    "   - Applies filters (e.g., 3×3, 5×5) to detect patterns\n",
    "   - Creates feature maps (one per filter)\n",
    "   - Detects edges, textures, shapes\n",
    "\n",
    "2. ReLU Activation: Introduces non-linearity\n",
    "   - `f(x) = max(0, x)`\n",
    "   - Sets negative values to 0\n",
    "   - Helps network learn complex patterns\n",
    "\n",
    "3. Pooling Layer: Down-samples feature maps\n",
    "   - Reduces spatial dimensions\n",
    "   - Max pooling: takes maximum value in a region\n",
    "   - Makes features more robust to small translations\n",
    "\n",
    "4. Fully Connected (Dense) Layer: Final classification\n",
    "   - Flattens 2D feature maps to 1D vector\n",
    "   - Connects to output classes\n",
    "\n",
    "CNN Workflow:\n",
    "Input Image → Conv + ReLU → Pooling → Conv + ReLU → Pooling → Flatten → Dense → Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4818dd",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning\n",
    "\n",
    "Transfer Learning reuses a model trained on one task (ImageNet) for a different task (clothing classification).\n",
    "\n",
    "Approach:\n",
    "\n",
    "1. Load pre-trained model (feature extractor)\n",
    "2. Remove original classification head\n",
    "3. Freeze convolutional layers\n",
    "4. Add custom layers for our task\n",
    "5. Train only the new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6279dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Preprocessing\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)), # no default needed, as training new model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ClothingDataset(\n",
    "    data_dir='./clothing-dataset-small/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = ClothingDataset(\n",
    "    data_dir='./clothing-dataset-small/validation',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(1280, num_classes) # new layer to be trained\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148dfd59",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7396cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ClothingClassifierMobileNet(num_classes=10)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14208c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss() # multi-class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb10ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model, optimizer, \n",
    "    train_loader, val_loader, \n",
    "    criterion, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Iterate over the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to prevent accumulation\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training loss\n",
    "            running_loss += loss.item()\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # Disable gradient calculation for validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move data to the specified device (GPU or CPU)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        #saves the model during training to:\n",
    "        #- Keep the best performing model\n",
    "        #- Resume training if interrupted\n",
    "        #- Avoid losing progress\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'clothing_v4_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87811d1",
   "metadata": {},
   "source": [
    "## 6. Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c12ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01):\n",
    "    model = ClothingClassifierMobileNet(num_classes=10)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c46ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for lr in [0.001, 0.1]: #0.01, 0.1]:\n",
    "  print(\"learning rate =\", lr)\n",
    "  model, optimizer = make_model(lr)\n",
    "  train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e03fa9",
   "metadata": {},
   "source": [
    "## 8. Adding Inner Layers\n",
    "\n",
    "intermediate dense layers between feature extraction and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187a467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42599a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model, optimizer = make_model(\n",
    "    learning_rate=0.001,\n",
    "    size_inner=100,\n",
    "    droprate=0.2,\n",
    ")\n",
    "# Add custom layers\n",
    "train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bddff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.001, size_inner=100):\n",
    "    model = ClothingClassifierMobileNet(\n",
    "        num_classes=10,\n",
    "        size_inner=size_inner # new number of inner layers\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce1d2c",
   "metadata": {},
   "source": [
    "## 9. Dropout Regularization\n",
    "Dropout randomly drops neurons during training to prevent overfitting.\n",
    "\n",
    "How it works:\n",
    "- Training: randomly set fraction of activations to 0\n",
    "- Inference: use all neurons (dropout disabled automatically)\n",
    "- Creates ensemble effect\n",
    "\n",
    "Benefits:\n",
    "- Prevents relying on specific features\n",
    "- Forces learning robust patterns\n",
    "- Reduces overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2,  num_classes=10):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU() # new activation layer\n",
    "        self.dropout = nn.Dropout(droprate)  # Add dropout\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x) # new inner layer\n",
    "        x = self.relu(x) # new activation layer\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f901161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        learning_rate=0.001,\n",
    "        size_inner=100, # new number of inner layers\n",
    "        droprate=0.2 # proportion of neurons to drop\n",
    "):\n",
    "    model = ClothingClassifierMobileNet(\n",
    "        num_classes=10,\n",
    "        size_inner=size_inner,\n",
    "        droprate=droprate\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ed625",
   "metadata": {},
   "source": [
    "## 10. Data Augmentation\n",
    "\n",
    "Data Augmentation artificially increases dataset size by applying random transformations to training images.\n",
    "\n",
    "Common transformations:\n",
    "- Rotation\n",
    "- Horizontal/vertical flipping\n",
    "- Zooming (random cropping)\n",
    "- Shifting\n",
    "- Shearing\n",
    "\n",
    "Important rules:\n",
    "- ✅ Apply ONLY to training data\n",
    "- ❌ Never augment validation/test data\n",
    "\n",
    "### Augmented Training Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms WITH augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Validation transforms - NO augmentation, same as before\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88262e37",
   "metadata": {},
   "source": [
    "## 11. Using the Trained Model\n",
    "\n",
    "### Loading a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ddae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/clothing_v4_04_0.812.pth'\n",
    "\n",
    "# Load model\n",
    "model = ClothingClassifierMobileNet(size_inner=32, droprate=0.2, num_classes=10)\n",
    "model.load_state_dict(torch.load(latest_file))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = val_transforms(img) #preproccess\n",
    "batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(classes, output[0].to('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15c414",
   "metadata": {},
   "source": [
    "## 12. Exporting to ONNX\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is a format for model interoperability.\n",
    "\n",
    "Benefits:\n",
    "- Deploy on different platforms\n",
    "- Use optimized runtimes (ONNX Runtime)\n",
    "- Better inference performance\n",
    "- Language-agnostic deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"clothing_classifier_mobilenet_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    verbose=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
